# ğŸ§  ML_learning: Machine Learning Model Collection

This repository provides a curated, modular collection of classical machine learning models implemented in Python. It is structured for both **educational purposes** and **project prototyping**, covering:

- âœ… Supervised learning: classification and regression
- âœ… Unsupervised learning: clustering, dimensionality reduction, anomaly detection
- âœ… Reusable pipeline modules for clean preprocessing


## Project Introduction

    ML_learning/
    â”œâ”€â”€ model/
    â”‚ â”œâ”€â”€ supervised_model/
    â”‚ â”‚ â”œâ”€â”€ classification_model/
    â”‚ â”‚ â”‚ â”œâ”€â”€ classification_data/
    â”‚ â”‚ â”‚ â”‚ â””â”€â”€ bank/
    â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ bank.csv
    â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ bank-full.csv
    â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ bank-names.txt
    â”‚ â”‚ â”‚ â”œâ”€â”€ Logistic_Regression.ipynb
    â”‚ â”‚ â”‚ â”œâ”€â”€ Naive_Bayes.ipynb
    â”‚ â”‚ â”‚ â”œâ”€â”€ SVM.ipynb
    â”‚ â”‚ â”‚ â”œâ”€â”€ Tree_Classifier.ipynb
    â”‚ â”‚ â”‚ â””â”€â”€ data_pipeline.py
    â”‚ â”‚
    â”‚ â”‚ â”œâ”€â”€ regression_model/
    â”‚ â”‚ â”‚ â”œâ”€â”€ regression_data/
    â”‚ â”‚ â”‚ â”‚ â””â”€â”€ house_price/
    â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ train.csv
    â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ test.csv
    â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€ sample_submission.csv
    â”‚ â”‚ â”‚ â”‚ â””â”€â”€ data_description.txt
    â”‚ â”‚ â”‚ â””â”€â”€ Linear_Regression.ipynb
    â”‚
    â”‚ â”œâ”€â”€ unsupervised_model/
    â”‚ â”‚ â”œâ”€â”€ unsup_data/
    â”‚ â”‚ â”‚ â””â”€â”€ Country-data.csv
    â”‚ â”‚ â”œâ”€â”€ Kmeans.ipynb
    â”‚ â”‚ â””â”€â”€ unsup_data_pipeline.py
    â”‚ 
    â”‚ â”œâ”€â”€ deep_learning_model/
    â”‚
    â”œâ”€â”€ README.md


## ğŸ“Š Datasets Used

This repository uses publicly available datasets suitable for both supervised and unsupervised machine learning tasks:

### ğŸ§ª Supervised Learning
- **ğŸ“‚ bank.csv**  
  From the UCI Bank Marketing dataset.  
  Task: Binary classification (`y`: subscribed to term deposit).  
  Location: `classification_model/classification_data/bank/`

- **ğŸ“‚ house_price**  
  From Kaggleâ€™s House Prices: Advanced Regression Techniques.  
  Task: Regression (predicting house sale prices).  
  Location: `regression_model/regression_data/house_price/`

### ğŸ” Unsupervised Learning
- **ğŸ“‚ Country-data.csv**  
  From Kaggle: *Unsupervised Learning on Country Data*  
  Task: Clustering countries based on socio-economic indicators.  
  Location: `unsupervised_model/unsup_data/`

All datasets are preprocessed using custom pipeline scripts (`data_pipeline.py` and `unsup_data_pipeline.py`) and split into train/test if necessary.


## ğŸ”§ Pipeline Modules

To ensure consistent preprocessing across models, the project includes reusable pipeline modules built with `scikit-learn`â€™s `Pipeline` and `ColumnTransformer`.

For example:

### ğŸ“¦ `data_pipeline.py`
Used in: **classification & regression tasks**  
Located at: `classification_model/data_pipeline.py`

**Key Features:**
- Automatically encodes categorical columns with `OneHotEncoder`
- Scales numerical columns with `StandardScaler`
- Easily wraps any estimator (e.g., LogisticRegression, DecisionTree)

They works in similar for different model


## ğŸ“š Recommended Learning Order

To maximize learning and build on foundational concepts step by step, hereâ€™s a suggested progression:

### ğŸ”· Supervised Learning

#### ğŸ”¹ Classification Models
  1. `Logistic_Regression.ipynb`  
     â†’ Intro to classification and linear decision boundaries

  2. `Tree_Classifier.ipynb`  
     â†’ Covers Decision Trees, Random Forest, GBDT, XGBoost, LightGBM

  3. `SVM.ipynb`  
     â†’ Kernel-based margin classifiers

  4. `Naive_Bayes.ipynb`  
     â†’ Probabilistic modeling and conditional independence

#### ğŸ”¹ Regression Models
5. `Linear_Regression.ipynb`  
   â†’ Fundamental regression modeling (with optional regularization)

---

### ğŸ”· Unsupervised Learning

6. `Kmeans.ipynb`  
   â†’ Introduction to clustering using K-means and the elbow method

(*More models such as PCA, DBSCAN, and anomaly detection can follow here*)

---

Each notebook builds conceptually on the last, and all preprocessing is modularized using pipelines for reusability across experiments.


