{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 🔗 MLP（Multi-Layer Perceptron 多层感知机）简介\n",
    "\n",
    "多层感知机（MLP）是最基本的前馈神经网络结构，是现代神经网络和深度学习的基础。它通过多个线性层和非线性激活函数的组合，能够对输入数据建模出复杂的非线性关系。\n",
    "\n",
    "MLP 通常用于：\n",
    "- 分类任务（如手写数字识别、客户流失预测）\n",
    "- 回归任务（如房价预测）\n",
    "- 表格型结构化数据（非图像、非序列）\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 基本结构\n",
    "\n",
    "一个典型的 MLP 包括：\n",
    "\n",
    "- **输入层**：接收原始特征向量\n",
    "- **1 个或多个隐藏层**：提取特征、建模复杂模式\n",
    "- **输出层**：给出最终的预测结果（如分类概率）\n",
    "\n",
    "每一层都执行：\n",
    "$$\n",
    "z = W · x + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = activation(z)\n",
    "$$\n",
    "\n",
    "\n",
    "其中：\n",
    "- `W`: 权重矩阵\n",
    "- `b`: 偏置向量\n",
    "- `activation()`: 激活函数（如 ReLU、Sigmoid）(可以参考 basic/model_building_blocks/Activation_function.ipynb）\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 MLP 引入的新概念\n",
    "\n",
    "| 概念             | 解释 |\n",
    "|------------------|------|\n",
    "| **神经元 (Neuron)**       | 模拟生物神经元的计算单元，执行 $z = w^\\top x + b$ |\n",
    "| **激活函数 (Activation)** | 给网络增加非线性能力。常用：ReLU、Sigmoid、Tanh |\n",
    "| **隐藏层 (Hidden Layer)** | 输入层和输出层之间的中间层，用于提取特征 |\n",
    "| **前向传播 (Forward Propagation)** | 从输入依次计算每层输出直到最终结果 |\n",
    "| **反向传播 (Backpropagation)**     | 利用损失函数计算梯度，更新网络权重 |\n",
    "| **优化器 (Optimizer)**   | 控制参数更新方式，如 SGD、Adam |\n",
    "| **损失函数 (Loss Function)** | 衡量预测与真实标签的差异，指导模型学习 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 MLP 与逻辑回归的关系\n",
    "\n",
    "- 逻辑回归 = 没有隐藏层的 MLP\n",
    "- MLP = 多层非线性组合的逻辑回归\n",
    "- 逻辑回归只能学习线性边界，而 MLP 可以学习任意复杂边界（由层数和激活函数决定）\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 应用场景示例\n",
    "\n",
    "- 银行客户是否会购买理财产品（分类）\n",
    "- 预测股票价格（回归）\n",
    "- 神经网络中最常用的基础结构（NLP、CV前身）\n",
    "\n"
   ],
   "metadata": {
    "id": "XYNqkZhTIOg4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🔗 多层感知机（MLP）：网络结构与前向传播\n",
    "\n",
    "## 🧬 1. 神经元结构：线性变换 + 激活函数\n",
    "\n",
    "每个神经元（Neuron）的核心操作可以表示为：\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "- $\\mathbf{x}$：输入向量\n",
    "- $\\mathbf{w}$：权重向量\n",
    "- $b$：偏置项\n",
    "- $z$：加权和\n",
    "\n",
    "然后将 $z$ 送入激活函数 $f(\\cdot)$ 得到输出：\n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ 2. 网络层结构：输入层 → 隐藏层 → 输出层\n",
    "\n",
    "一个标准的 MLP 网络包含：\n",
    "\n",
    "- **输入层**：接收特征向量（不计算）\n",
    "- **一个或多个隐藏层**：每层含若干神经元\n",
    "- **输出层**：用于输出预测结果\n",
    "\n",
    "例如一个两层 MLP 结构如下：\n",
    "\n",
    "$$\n",
    "\\text{输入层：} \\quad \\mathbf{x} \\in \\mathbb{R}^n \\\\\n",
    "\\text{隐藏层：} \\quad \\mathbf{a}^{(1)} = f(W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) \\\\\n",
    "\\text{输出层：} \\quad \\hat{\\mathbf{y}} = g(W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 3. 前向传播公式（矩阵向量形式）\n",
    "\n",
    "以一个隐藏层为例，完整前向传播过程为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}^{(1)} &= W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\\\\n",
    "\\mathbf{a}^{(1)} &= f(\\mathbf{z}^{(1)}) \\\\\n",
    "\\mathbf{z}^{(2)} &= W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} \\\\\n",
    "\\hat{\\mathbf{y}} &= g(\\mathbf{z}^{(2)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> 若有多隐藏层，只需重复上述结构。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 4. 输出形式差异（分类 vs 回归）\n",
    "\n",
    "| 任务类型       | 输出层激活函数 $g(z)$ | 典型损失函数               | 输出范围         |\n",
    "|----------------|------------------------|----------------------------|------------------|\n",
    "| 二分类         | Sigmoid                | Binary Cross-Entropy       | (0, 1) 概率       |\n",
    "| 多分类         | Softmax                | Categorical Cross-Entropy  | 各类概率分布     |\n",
    "| 回归           | Linear (恒等映射)      | MSE / MAE                  | 实数域            |\n",
    "\n",
    "- **Sigmoid** 用于将输出压缩为概率\n",
    "- **Softmax** 用于多类分类，输出为概率分布\n",
    "- **Linear** 保持数值连续性，适用于回归任务\n"
   ],
   "metadata": {
    "id": "Ku3I0GHEJREF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🔌 激活函数与权重初始化（Activation & Initialization）\n",
    "\n",
    "在多层感知机（MLP）中，激活函数用于为模型引入非线性能力，而权重初始化则影响模型的训练效率与稳定性。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 1. 为什么需要激活函数？\n",
    "\n",
    "如果没有激活函数，多个线性层叠加仍然是线性的，MLP 退化为线性模型：\n",
    "\n",
    "$$\n",
    "f(x) = W_2(W_1 x + b_1) + b_2 = W x + b\n",
    "$$\n",
    "\n",
    "> 因此激活函数必须非线性，才能让 MLP 学习复杂的函数关系。\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ 2. 常见激活函数对比\n",
    "\n",
    "| 激活函数 | 表达式 | 输出范围 | 是否中心化 | 是否稀疏 | 优点 | 缺点 |\n",
    "|----------|--------|----------|--------------|------------|------|------|\n",
    "| Sigmoid | $\\frac{1}{1 + e^{-x}}$ | (0, 1) | ❌ | ❌ | 可导、概率输出 | 梯度消失、输出非零中心 |\n",
    "| Tanh | $\\tanh(x)$ | (-1, 1) | ✅ | ❌ | 输出零中心，梯度大于 sigmoid | 梯度仍可能消失 |\n",
    "| ReLU | $\\max(0, x)$ | [0, ∞) | ❌ | ✅（部分输出为 0） | 收敛快、计算简单 | Dying ReLU 问题 |\n",
    "| Leaky ReLU | $\\max(\\alpha x, x)$ | (-∞, ∞) | ❌ | ✅ | 缓解 Dying ReLU 问题 | α 需要调参 |\n",
    "| GELU / Swish | 自适应平滑 | (-∞, ∞) | ✅ | ❌ | 新一代激活，效果好 | 计算较慢 |\n",
    "\n",
    "具体可以参考basic_concept中激活函数部分\n",
    "---\n",
    "\n",
    "## 🔧 3. 为什么初始化很重要？\n",
    "\n",
    "- 不恰当的初始化可能导致：\n",
    "  - 梯度消失或爆炸（训练不收敛）\n",
    "  - 所有神经元行为一致（无学习）\n",
    "\n",
    "初始化目标：\n",
    "> 保持每层输出的方差稳定，避免信号在传播过程中变大或变小\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ 4. 常见初始化方法\n",
    "\n",
    "| 初始化方法 | 原理 | 推荐激活 | 数学表达 |\n",
    "|------------|------|----------|-----------|\n",
    "| 常规均匀分布 | 固定范围随机值 | 早期方法 | $U(-0.1, 0.1)$ |\n",
    "| Xavier 初始化 | 保持前后方差一致 | Sigmoid / Tanh | $\\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})$ |\n",
    "| He 初始化 | 考虑 ReLU 截断 | ReLU 系列 | $\\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})$ |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 小结\n",
    "\n",
    "- 激活函数决定了模型的非线性能力，是深度学习的核心部分之一\n",
    "- 常用 ReLU 作为默认激活函数，也可尝试 Swish/GELU 等\n",
    "- 权重初始化需与激活函数匹配，如使用 ReLU 建议用 He 初始化\n",
    "\n"
   ],
   "metadata": {
    "id": "G9q1bTgOKPzn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y3gM6n80IKbj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 数据预处理：转换为Tensor & 标准化\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量 (0~1)\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 均值 & 标准差\n",
    "])\n",
    "\n",
    "# 下载训练和测试集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4k0svYRTOUCX",
    "outputId": "ff1bc954-f489-49cf-b8bb-d41e4f6b4ff9"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 56.4MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.65MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 13.9MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.88MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),             # 28x28 -> 784\n",
    "            nn.Linear(784, 128),      # 输入层 → 隐藏层1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),       # 隐藏层1 → 隐藏层2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)         # 隐藏层2 → 输出层（10类）\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "metadata": {
    "id": "T4-F1bSCOWLh"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()              # 自动加 softmax + one-hot loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ],
   "metadata": {
    "id": "hr4mkPZEOYZM"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # 反向传播 + 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loader.dataset)}]  Loss: {loss.item():.4f}\")\n"
   ],
   "metadata": {
    "id": "ePPvD679OaUk"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ],
   "metadata": {
    "id": "o8U9Slv8Obwm"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(1, 6):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, test_loader)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICMRAaLUOmgz",
    "outputId": "8d462248-9266-4253-f7b4-8fb7f0a22951"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000]  Loss: 2.2964\n",
      "Train Epoch: 1 [6400/60000]  Loss: 0.4901\n",
      "Train Epoch: 1 [12800/60000]  Loss: 0.3854\n",
      "Train Epoch: 1 [19200/60000]  Loss: 0.2871\n",
      "Train Epoch: 1 [25600/60000]  Loss: 0.2668\n",
      "Train Epoch: 1 [32000/60000]  Loss: 0.2352\n",
      "Train Epoch: 1 [38400/60000]  Loss: 0.1647\n",
      "Train Epoch: 1 [44800/60000]  Loss: 0.0816\n",
      "Train Epoch: 1 [51200/60000]  Loss: 0.1236\n",
      "Train Epoch: 1 [57600/60000]  Loss: 0.1558\n",
      "Test Accuracy: 95.40%\n",
      "Train Epoch: 2 [0/60000]  Loss: 0.1232\n",
      "Train Epoch: 2 [6400/60000]  Loss: 0.1517\n",
      "Train Epoch: 2 [12800/60000]  Loss: 0.1374\n",
      "Train Epoch: 2 [19200/60000]  Loss: 0.2623\n",
      "Train Epoch: 2 [25600/60000]  Loss: 0.0645\n",
      "Train Epoch: 2 [32000/60000]  Loss: 0.0887\n",
      "Train Epoch: 2 [38400/60000]  Loss: 0.3113\n"
     ]
    }
   ]
  }
 ]
}
