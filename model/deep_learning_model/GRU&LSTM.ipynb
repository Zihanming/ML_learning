{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🔄 GRU：门控循环单元（Gated Recurrent Unit）\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 一、为什么使用 GRU？\n",
        "\n",
        "RNN 在面对长序列时，容易遇到两个问题：\n",
        "\n",
        "- ❌ 梯度消失/爆炸，导致无法有效学习长期依赖；\n",
        "- ❌ 每次状态完全覆盖，难以“记住”和“忘记”特定信息。\n",
        "\n",
        "✅ GRU 引入了两种门机制：\n",
        "- 更新门（update gate）：控制保留多少旧信息；\n",
        "- 重置门（reset gate）：控制整合多少旧信息。\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 二、GRU 的公式结构（D2L 版本）\n",
        "\n",
        "设：\n",
        "- 输入为 $X_t \\in \\mathbb{R}^d$\n",
        "- 上一隐藏状态为 $H_{t-1} \\in \\mathbb{R}^h$\n",
        "\n",
        "GRU 结构如下：\n",
        "\n",
        "### 1️⃣ 更新门（update gate）\n",
        "$$\n",
        "Z_t = \\sigma(X_t W_{xz} + H_{t-1} W_{hz} + b_z)\n",
        "$$\n",
        "\n",
        "### 2️⃣ 重置门（reset gate）\n",
        "$$\n",
        "R_t = \\sigma(X_t W_{xr} + H_{t-1} W_{hr} + b_r)\n",
        "$$\n",
        "\n",
        "### 3️⃣ 候选隐藏状态（candidate activation）\n",
        "$$\n",
        "\\tilde{H}_t = \\tanh(X_t W_{xh} + (R_t \\odot H_{t-1}) W_{hh} + b_h)\n",
        "$$\n",
        "\n",
        "注意这里的 $R_t \\odot H_{t-1}$ 表示：\n",
        "- **根据重置门抑制旧状态的一部分信息**，再与当前输入组合\n",
        "\n",
        "### 4️⃣ 当前隐藏状态（最终输出）\n",
        "$$\n",
        "H_t = (1 - Z_t) \\odot H_{t-1} + Z_t \\odot \\tilde{H}_t\n",
        "$$\n",
        "\n",
        "含义：\n",
        "- 更新门决定有多少保留旧状态，多少用新信息替代。\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 🏗️ 三、GRU 信息流结构图\n",
        "\n",
        "               ┌─────────────┐\n",
        "               │ xₜ          │\n",
        "               └────┬────────┘\n",
        "                    │\n",
        "               ┌────▼────┐\n",
        "               │ 拼接 [hₜ₋₁, xₜ] │\n",
        "               └────┬────┘\n",
        "    ┌──────────────┼──────────────┐\n",
        "    ▼              ▼              ▼\n",
        "┌──────┐       ┌──────┐      ┌─────────────┐\n",
        "│ zₜ   │       │ rₜ   │      │ 重置状态 rₜ·hₜ₋₁ │\n",
        "│sigmoid│       │sigmoid│      └─────────────┘\n",
        "└──┬───┘       └──┬───┘             │\n",
        "   │              │                ▼\n",
        "   │              └────┐     ┌─────────────┐\n",
        "   ▼                   └────▶│  候选状态 h̃ₜ  │\n",
        "                          ┌──▶│ tanh(...)    │\n",
        "                          │   └────┬────────┘\n",
        "                          ▼        ▼\n",
        "                  ┌─────────────────────────────┐\n",
        "                  │ hₜ = (1 - zₜ)·hₜ₋₁ + zₜ·h̃ₜ │\n",
        "                  └─────────────────────────────┘\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 四、GRU 与 LSTM 的对比\n",
        "\n",
        "| 对比项       | GRU                          | LSTM                         |\n",
        "|--------------|-------------------------------|------------------------------|\n",
        "| 门控数量     | 2 个（更新门、重置门）        | 3 个（遗忘、输入、输出）      |\n",
        "| 记忆结构     | 只有 $h_t$                   | 有 $h_t$ 和 $C_t$             |\n",
        "| 参数数量     | 少                           | 多                           |\n",
        "| 收敛速度     | 快                           | 略慢                         |\n",
        "| 性能表现     | 多任务中性能不逊色            | 稳定性略优（长序列任务）       |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 五、使用建议\n",
        "\n",
        "- **GRU 更轻量、计算更快**，适合快速迭代和中短序列任务\n",
        "- 如果你不确定用哪个，GRU 是个好起点\n",
        "- 对长期依赖特别强的任务可优先考虑 LSTM\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 常见 GRU 面试问题汇总与解答\n",
        "\n",
        "---\n",
        "\n",
        "### ❓1. GRU 有哪些门？各自的作用是什么？\n",
        "\n",
        "GRU 有两个门控机制：\n",
        "\n",
        "| 门名称 | 数学公式 | 功能解释 |\n",
        "|--------|-----------|----------|\n",
        "| 更新门 $z_t$ | $z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)$ | 控制当前隐藏状态中，保留多少旧状态 $h_{t-1}$，引入多少新状态 |\n",
        "| 重置门 $r_t$ | $r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$ | 控制在生成候选状态 $\\tilde{h}_t$ 时，遗忘多少旧状态信息 |\n",
        "\n",
        "✅ 总结：\n",
        "- **更新门**决定记忆多少旧信息\n",
        "- **重置门**决定整合多少旧信息用于当前判断\n",
        "\n",
        "---\n",
        "\n",
        "### ❓2. GRU 为什么比 RNN 强？它解决了什么问题？\n",
        "\n",
        "GRU 比传统 RNN 更强的原因有：\n",
        "\n",
        "- ✅ **门控机制**解决了 RNN 的梯度消失问题\n",
        "- ✅ 能 **动态记忆/遗忘** 长期依赖信息\n",
        "- ✅ 相比 RNN 更容易收敛，性能稳定\n",
        "\n",
        "它的改进点：\n",
        "\n",
        "- RNN 的隐藏状态简单叠加，容易丢失远程依赖\n",
        "- GRU 通过门控控制记忆流动，**保留有用信息、抑制无关信息**\n",
        "\n",
        "---\n",
        "\n",
        "### ❓3. GRU 和 LSTM 各自适合什么任务？\n",
        "\n",
        "| 模型 | 特性 | 更适合的任务类型 |\n",
        "|------|------|------------------|\n",
        "| **GRU** | 简洁高效，训练快，参数少 | 资源受限、快速迭代、文本分类、语音识别 |\n",
        "| **LSTM** | 有独立记忆单元，长期依赖建模能力强 | 长序列任务，如翻译、语言建模、对话生成 |\n",
        "\n",
        "✅ 经验法则：\n",
        "- 如果 **速度/内存重要**，用 GRU\n",
        "- 如果任务 **依赖长距离上下文**，优先用 LSTM\n",
        "\n",
        "---\n",
        "\n",
        "### ❓4. 为什么 GRU 没有输出门也能工作？\n",
        "\n",
        "- LSTM 使用输出门 $o_t$ 控制 $C_t$ 对 $h_t$ 的影响\n",
        "- GRU 中没有显式记忆单元 $C_t$，其状态更新为：\n",
        "  \n",
        "  $$\n",
        "  h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t\n",
        "  $$\n",
        "\n",
        "- 更新门 $z_t$ 本身已起到“输出控制”的作用\n",
        "\n",
        "✅ 结论：\n",
        "> GRU 的结构已将“记忆更新 + 输出调节”整合在一起，**无需单独的输出门**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 📚 七、PyTorch 中使用方式\n",
        "\n",
        "```python\n",
        "rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1)\n",
        "output, hn = rnn(input_seq, h0)\n"
      ],
      "metadata": {
        "id": "WwCBzViL2sib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------\n",
        "# 手写 GRU\n",
        "# ---------------------\n",
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_xz = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_xr = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        z_t = torch.sigmoid(self.W_xz(x_t) + self.W_hz(h_prev))\n",
        "        r_t = torch.sigmoid(self.W_xr(x_t) + self.W_hr(h_prev))\n",
        "        h_tilde = torch.tanh(self.W_xh(x_t) + self.W_hh(r_t * h_prev))\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
        "        return h_t\n",
        "\n",
        "class ManualGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.cell = GRUCell(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs, h0=None):\n",
        "        seq_len, batch_size, _ = inputs.shape\n",
        "        hidden_size = self.cell.hidden_size\n",
        "        if h0 is None:\n",
        "            h0 = torch.zeros(batch_size, hidden_size, device=inputs.device)\n",
        "        outputs = []\n",
        "        h = h0\n",
        "        for t in range(seq_len):\n",
        "            h = self.cell(inputs[t], h)\n",
        "            outputs.append(h.unsqueeze(0))\n",
        "        return torch.cat(outputs, dim=0), h.unsqueeze(0)"
      ],
      "metadata": {
        "id": "lG-3iXiB9osx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# 模拟输入\n",
        "# ---------------------\n",
        "seq_len = 50\n",
        "batch_size = 64\n",
        "input_size = 32\n",
        "hidden_size = 64\n",
        "x = torch.randn(seq_len, batch_size, input_size)\n",
        "\n",
        "# ---------------------\n",
        "# 模型定义与包装\n",
        "# ---------------------\n",
        "manual_gru = ManualGRU(input_size, hidden_size)\n",
        "builtin_gru = nn.GRU(input_size, hidden_size)\n",
        "\n",
        "class GRUWrapper(nn.Module):\n",
        "    def __init__(self, gru):\n",
        "        super().__init__()\n",
        "        self.gru = gru\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gru(x)[0]\n",
        "\n",
        "builtin_gru_wrapped = GRUWrapper(builtin_gru)\n",
        "\n",
        "# ---------------------\n",
        "# Benchmark 函数\n",
        "# ---------------------\n",
        "def benchmark(model, name, inputs, repeat=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        for _ in range(repeat):\n",
        "            _ = model(inputs)\n",
        "        end = time.time()\n",
        "    return name, end - start\n",
        "\n",
        "# ---------------------\n",
        "# Benchmark 运行\n",
        "# ---------------------\n",
        "results = [\n",
        "    benchmark(builtin_gru_wrapped, \"nn.GRU\", x),\n",
        "    benchmark(manual_gru, \"Manual GRU\", x)\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"Model\", \"Time (s)\"])\n",
        "print(df)\n",
        "\n",
        "# ---------------------\n",
        "# 输出误差与参数量对比\n",
        "# ---------------------\n",
        "out_builtin, _ = builtin_gru(x)\n",
        "out_manual, _ = manual_gru(x)\n",
        "\n",
        "max_diff = (out_builtin - out_manual).abs().max().item()\n",
        "mean_diff = (out_builtin - out_manual).abs().mean().item()\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_builtin = count_parameters(builtin_gru)\n",
        "n_manual = count_parameters(manual_gru)\n",
        "\n",
        "print(f\"\\n🔍 Output max diff: {max_diff:.6f}\")\n",
        "print(f\"🔍 Output mean diff: {mean_diff:.6f}\")\n",
        "print(f\"🧠 Builtin GRU params: {n_builtin}\")\n",
        "print(f\"🧠 Manual  GRU params: {n_manual}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB85rWJm691v",
        "outputId": "830ae34b-123a-4763-a8c2-cad5380297a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Model  Time (s)\n",
            "0      nn.GRU  0.076545\n",
            "1  Manual GRU  0.124358\n",
            "\n",
            "🔍 Output max diff: 1.447857\n",
            "🔍 Output mean diff: 0.304520\n",
            "🧠 Builtin GRU params: 18816\n",
            "🧠 Manual  GRU params: 18816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 LSTM：长短期记忆网络（Long Short-Term Memory）\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 一、为什么要引入 LSTM？\n",
        "\n",
        "传统 RNN 存在两个核心问题：\n",
        "\n",
        "- **梯度消失或爆炸**：长序列中早期信息在传播过程中几乎消失，导致学习失败。\n",
        "- **难以建模长期依赖**：RNN 容易被“短期上下文”干扰，难以保留长期记忆。\n",
        "\n",
        "✅ LSTM 通过引入“**门控机制**”来控制信息的保留、遗忘与更新，缓解这些问题。\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 二、LSTM 的结构公式\n",
        "\n",
        "LSTM 的每个时间步包含 4 个主要门：\n",
        "\n",
        "> 所有门都依赖于前一时刻的隐藏状态 $h_{t-1}$ 和当前输入 $x_t$\n",
        "\n",
        "- **遗忘门** $f_t$：\n",
        "  $$\n",
        "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "  $$\n",
        "  控制保留多少旧记忆 $C_{t-1}$\n",
        "\n",
        "- **输入门** $i_t$：\n",
        "  $$\n",
        "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "  $$\n",
        "  控制是否写入新的记忆内容\n",
        "\n",
        "- **候选记忆** $\\tilde{C}_t$：\n",
        "  $$\n",
        "  \\tilde{C}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
        "  $$\n",
        "\n",
        "- **更新记忆单元**：\n",
        "  $$\n",
        "  C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
        "  $$\n",
        "\n",
        "- **输出门** $o_t$：\n",
        "  $$\n",
        "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "  $$\n",
        "\n",
        "- **当前隐藏状态** $h_t$：\n",
        "  $$\n",
        "  h_t = o_t \\odot \\tanh(C_t)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 🏗️ 三、LSTM 单元结构图（信息流）\n",
        "\n",
        "                         ┌─────────────┐\n",
        "                         │  xₜ         │\n",
        "                         └────┬────────┘\n",
        "                              │\n",
        "                              ▼\n",
        "                         ┌─────────────┐\n",
        "                         │ 拼接 [hₜ₋₁, xₜ] │\n",
        "                         └────┬────────┘\n",
        "                              │\n",
        "            ┌────────────────┼─────────────────────┐\n",
        "            ▼                ▼                     ▼\n",
        "        ┌───────┐       ┌────────┐            ┌────────┐\n",
        "        │ fₜ    │       │ iₜ     │            │ oₜ     │\n",
        "        │sigmoid│       │sigmoid │            │sigmoid │\n",
        "        └──┬────┘       └──┬─────┘            └──┬─────┘\n",
        "           │               │                    │\n",
        "           │               ▼                    │\n",
        "     ┌─────▼─────┐   ┌─────────────┐            │\n",
        "     │ Cₜ₋₁ × fₜ │   │  C̃ₜ = tanh(·) │◄────────┘\n",
        "     └─────┬─────┘   └─────────────┘\n",
        "           │               │\n",
        "           └─────┬─────────┘\n",
        "                 ▼\n",
        "           ┌────────────┐\n",
        "           │ Cₜ = fₜ·Cₜ₋₁ + iₜ·C̃ₜ │\n",
        "           └────┬───────┘\n",
        "                ▼\n",
        "          ┌────────────┐\n",
        "          │ hₜ = oₜ × tanh(Cₜ) │\n",
        "          └────────────┘\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ✏️ 四、与 RNN 的区别\n",
        "\n",
        "| 模型 | 状态结构 | 是否有记忆单元 C | 是否有门控 | 长期依赖建模能力 |\n",
        "|------|----------|------------------|------------|------------------|\n",
        "| RNN  | $h_t$    | ❌ 无             | ❌ 无       | 弱               |\n",
        "| LSTM | $h_t, C_t$ | ✅ 有             | ✅ 有       | 强               |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 五、LSTM 的优点与缺点\n",
        "\n",
        "### ✅ 优点：\n",
        "- 缓解梯度消失，能建模长依赖\n",
        "- 学习何时记、何时忘，鲁棒性强\n",
        "\n",
        "### ❌ 缺点：\n",
        "- 参数量大，计算开销高\n",
        "- 每步都要计算多个门，速度慢于 GRU/Transformer\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 六、常见应用场景\n",
        "\n",
        "- 文本生成、机器翻译（早期 Seq2Seq）\n",
        "- 时间序列预测（如股票/天气）\n",
        "- 情感分析、语音识别\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 七、面试常问\n",
        "\n",
        "- 为什么 RNN 不能捕捉长依赖？LSTM 怎么解决的？\n",
        "- LSTM 有几个门？各自作用？\n",
        "- LSTM 的参数量和 RNN 相比有什么变化？\n",
        "- LSTM 中的 $C_t$ 和 $h_t$ 各代表什么？\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 八、梯度裁剪（补充）\n",
        "\n",
        "由于 LSTM 仍可能在长序列中产生梯度爆炸，因此训练时经常使用：\n",
        "\n",
        "```python\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
      ],
      "metadata": {
        "id": "u_NFZl7W14iF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NGL5nvem1sK4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------\n",
        "# 手写 LSTM\n",
        "# ---------------------\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_xi = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hi = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_xf = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hf = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_xo = nn.Linear(input_size, hidden_size)\n",
        "        self.W_ho = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W_xc = nn.Linear(input_size, hidden_size)\n",
        "        self.W_hc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x_t, h_prev, c_prev):\n",
        "        i_t = torch.sigmoid(self.W_xi(x_t) + self.W_hi(h_prev))\n",
        "        f_t = torch.sigmoid(self.W_xf(x_t) + self.W_hf(h_prev))\n",
        "        o_t = torch.sigmoid(self.W_xo(x_t) + self.W_ho(h_prev))\n",
        "        c_tilde = torch.tanh(self.W_xc(x_t) + self.W_hc(h_prev))\n",
        "        c_t = f_t * c_prev + i_t * c_tilde\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "        return h_t, c_t\n",
        "\n",
        "class ManualLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.cell = LSTMCell(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs, h0=None, c0=None):\n",
        "        seq_len, batch_size, _ = inputs.shape\n",
        "        hidden_size = self.cell.hidden_size\n",
        "        if h0 is None:\n",
        "            h0 = torch.zeros(batch_size, hidden_size, device=inputs.device)\n",
        "        if c0 is None:\n",
        "            c0 = torch.zeros(batch_size, hidden_size, device=inputs.device)\n",
        "        outputs = []\n",
        "        h, c = h0, c0\n",
        "        for t in range(seq_len):\n",
        "            h, c = self.cell(inputs[t], h, c)\n",
        "            outputs.append(h.unsqueeze(0))\n",
        "        return torch.cat(outputs, dim=0), (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# Benchmark Setup\n",
        "# ---------------------\n",
        "def benchmark(model, name, inputs, repeat=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        for _ in range(repeat):\n",
        "            _ = model(inputs)\n",
        "        end = time.time()\n",
        "    return name, end - start\n",
        "\n",
        "# ---------------------\n",
        "# 模拟输入数据\n",
        "# ---------------------\n",
        "seq_len = 50\n",
        "batch_size = 64\n",
        "input_size = 32\n",
        "hidden_size = 64\n",
        "x = torch.randn(seq_len, batch_size, input_size)\n",
        "\n",
        "# ---------------------\n",
        "# 初始化模型\n",
        "# ---------------------\n",
        "manual_lstm = ManualLSTM(input_size, hidden_size)\n",
        "builtin_lstm = nn.LSTM(input_size, hidden_size)\n",
        "\n",
        "# 包一层 wrapper 以统一接口（仅返回 output）\n",
        "class LSTMWrapper(nn.Module):\n",
        "    def __init__(self, lstm):\n",
        "        super().__init__()\n",
        "        self.lstm = lstm\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lstm(x)[0]\n",
        "\n",
        "builtin_lstm_wrapped = LSTMWrapper(builtin_lstm)\n",
        "\n",
        "# ---------------------\n",
        "# 执行 Benchmark\n",
        "# ---------------------\n",
        "results = [\n",
        "    benchmark(builtin_lstm_wrapped, \"nn.LSTM\", x),\n",
        "    benchmark(manual_lstm, \"Manual LSTM\", x)\n",
        "]\n",
        "\n",
        "# 展示结果\n",
        "df = pd.DataFrame(results, columns=[\"Model\", \"Time (s)\"])\n",
        "print(df)\n",
        "\n",
        "# =======================\n",
        "# 对比输出误差 & 参数量\n",
        "# =======================\n",
        "out_builtin, _ = builtin_lstm(x)\n",
        "out_manual, _ = manual_lstm(x)\n",
        "\n",
        "# 输出误差（最大值、平均值）\n",
        "max_diff = (out_builtin - out_manual).abs().max().item()\n",
        "mean_diff = (out_builtin - out_manual).abs().mean().item()\n",
        "\n",
        "# 参数量统计\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_builtin = count_parameters(builtin_lstm)\n",
        "n_manual = count_parameters(manual_lstm)\n",
        "\n",
        "print(f\"🔍 Output max diff: {max_diff:.6f}\")\n",
        "print(f\"🔍 Output mean diff: {mean_diff:.6f}\")\n",
        "print(f\"🧠 Builtin LSTM params: {n_builtin}\")\n",
        "print(f\"🧠 Manual  LSTM params: {n_manual}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY_Xo0dR8Vur",
        "outputId": "18f208aa-ff68-4e3c-f0f8-71444a3e4642"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Model  Time (s)\n",
            "0      nn.LSTM  0.077001\n",
            "1  Manual LSTM  0.152505\n",
            "🔍 Output max diff: 0.829677\n",
            "🔍 Output mean diff: 0.148258\n",
            "🧠 Builtin LSTM params: 25088\n",
            "🧠 Manual  LSTM params: 25088\n"
          ]
        }
      ]
    }
  ]
}