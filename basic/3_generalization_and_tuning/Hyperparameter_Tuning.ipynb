{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯 超参数调优简介（Hyperparameter Tuning Intro）\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 什么是超参数（Hyperparameter）？\n",
        "\n",
        "在机器学习中：\n",
        "\n",
        "- **参数（Parameter）**：模型在训练过程中自动学得（如权重、偏置）\n",
        "- **超参数（Hyperparameter）**：需要**在训练前人为指定**，控制模型训练行为\n",
        "\n",
        "> ✅ 参数是模型“学”的，超参数是我们“调”的。\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 常见超参数有哪些？\n",
        "\n",
        "| 模型类型     | 常见超参数                          |\n",
        "|------------|-----------------------------------|\n",
        "| 逻辑回归 / SVM | 正则化强度（C）、惩罚方式（L1/L2）       |\n",
        "| 决策树 / 随机森林 | 树深（max_depth）、样本数（min_samples_split） |\n",
        "| 神经网络     | 学习率、层数、隐藏单元数、Dropout比率        |\n",
        "| Boosting 系列 | 学习率、子样本比例、迭代轮数（n_estimators） |\n",
        "\n",
        "---\n",
        "\n",
        "## 📈 为什么需要调参？\n",
        "\n",
        "模型性能受到超参数显著影响：\n",
        "\n",
        "- 过大 → 过拟合（模型复杂、泛化差）\n",
        "- 过小 → 欠拟合（模型太弱、学习不动）\n",
        "- 学习率太大 → 发散；太小 → 收敛太慢\n",
        "- 正则太强 → 模型被限制；太弱 → 容易过拟合\n",
        "\n",
        "良好的调参可以：\n",
        "- ✅ 提升模型性能（更高 AUC / Accuracy / Recall 等）\n",
        "- ✅ 降低过拟合风险\n",
        "- ✅ 加速训练速度\n",
        "- ✅ 提高模型的稳定性与鲁棒性\n",
        "\n",
        "---\n",
        "\n",
        "## 🪜 调参流程概览\n",
        "\n",
        "```text\n",
        "确定模型 → 识别关键超参数 → 定义搜索空间 → 选择调参策略（Grid / Random / Bayesian） → 训练评估 → 选择最优模型 → 上线或再调优\n",
        "\n",
        "---\n",
        "\n",
        "## 面试常见问题（含答案）\n",
        "\n",
        "1. 什么是超参数？怎么与参数区分？\n",
        "\n",
        "    超参数需在训练前手动设定，控制训练过程；参数是训练中自动学习到的，如权重、偏置。\n",
        "\n",
        "2. 为什么要调超参数？\n",
        "\n",
        "    超参数控制模型复杂度与学习过程，合理设置能有效提升模型泛化能力和性能。\n",
        "\n",
        "3. 调参一般从哪些开始调？\n",
        "\n",
        "    通常先调最影响性能的（如学习率、正则项），再逐步微调其他参数。\n",
        "\n",
        "4. 调参有哪些方法？优缺点？\n",
        "\n",
        "    Grid Search（全面但慢）、Random Search（快但不完整）、Bayesian Optimization（智能高效但复杂）。"
      ],
      "metadata": {
        "id": "Bli1AzD5R7A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧮 Grid Search（网格搜索）\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 方法介绍\n",
        "\n",
        "Grid Search 是最基本的超参数调优方法：\n",
        "\n",
        "> 枚举所有可能的超参数组合，逐一训练模型并评估性能，最终选出最优组合。\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ 工作原理\n",
        "\n",
        "- 明确需要调参的超参数及其取值范围（如：C = [0.1, 1, 10]）\n",
        "- 构造所有可能组合（笛卡尔积）\n",
        "- 对每个组合训练模型并交叉验证（如 KFold）\n",
        "- 按照评估指标选择表现最佳的超参数组合作为最终模型\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 特性总结\n",
        "\n",
        "| 特性           | 描述说明                                      |\n",
        "|----------------|---------------------------------------------|\n",
        "| 搜索方式       | 穷举搜索：尝试所有参数组合                         |\n",
        "| 可并行计算     | ✅ 可以加速（n_jobs > 1）                     |\n",
        "| 计算开销       | ❌ 昂贵，组合越多越慢                              |\n",
        "| 可解释性       | ✅ 明确试过哪些参数                             |\n",
        "| 适用场景       | 超参数数量少、每个参数备选值不多的模型               |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 优点\n",
        "\n",
        "- 简单直观，容易理解与实现  \n",
        "- 适合调试阶段，小范围精准调参  \n",
        "- 可系统地遍历全部组合，结果可靠  \n",
        "- 与交叉验证结合，能有效避免过拟合\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 缺点\n",
        "\n",
        "- **组合爆炸**：$d$ 个参数每个 $k$ 个取值 → $k^d$ 个组合  \n",
        "- **冗余计算**：很多组合性能相近或冗余  \n",
        "- **效率低下**：无法利用历史信息指导搜索方向\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 常见应用\n",
        "\n",
        "- Logistic Regression / SVM（小模型）\n",
        "- 树模型参数初步粗调\n",
        "- 作为 baseline 方案对比\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 面试常见问题（含答案）\n",
        "\n",
        "1. **Grid Search 是什么？**\n",
        "   - 是穷举所有超参数组合并评估性能的一种调参方法。\n",
        "\n",
        "2. **优缺点分别是什么？**\n",
        "   - 优点是简单直观，组合全面；缺点是效率低、计算成本高。\n",
        "\n",
        "3. **适合用 Grid Search 的场景？**\n",
        "   - 超参数维度低、每个参数备选值不多时。\n",
        "\n",
        "4. **实际项目你调过哪些超参数？**\n",
        "   - 如 Ridge 的 alpha、SVM 的 C 与 kernel，配合 GridSearchCV 实现。\n",
        "\n",
        "5. **你如何控制 Grid Search 的开销？**\n",
        "   - 降低搜索空间维度、减少备选值数量、使用并行计算（n_jobs）、优先调重要参数。\n"
      ],
      "metadata": {
        "id": "LJdOloz2S8sv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxw4JM87QTPh",
        "outputId": "134fc526-319a-46a3-f906-f84126d1d714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "最优超参数 alpha: 10.0\n",
            "最优模型在验证集上的得分 (负MSE): -0.5192548258531767\n",
            "在测试集上的 MSE: 0.5550405537342994\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 加载数据\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 选择模型\n",
        "model = Ridge()\n",
        "\n",
        "# 定义参数网格\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "}\n",
        "\n",
        "# 初始化 GridSearch\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 训练并搜索\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 输出最佳超参数和评估\n",
        "print(\"最优超参数 alpha:\", grid.best_params_['alpha'])\n",
        "print(\"最优模型在验证集上的得分 (负MSE):\", grid.best_score_)\n",
        "\n",
        "# 在测试集上评估\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"在测试集上的 MSE:\", test_mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎲 Random Search（随机搜索）\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 方法介绍\n",
        "\n",
        "Random Search 是一种随机采样的调参方法：\n",
        "\n",
        "> 不再尝试所有超参数组合，而是**从参数空间中随机采样固定数量的组合**，用较少计算成本找到“足够好”的结果。\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ 工作原理\n",
        "\n",
        "- 为每个超参数设定搜索范围（可离散、可连续）\n",
        "- 随机采样 $n$ 组参数组合\n",
        "- 使用交叉验证评估每组性能\n",
        "- 返回表现最好的参数组合作为最优模型\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 特性总结\n",
        "\n",
        "| 特性           | 描述说明                                  |\n",
        "|----------------|-------------------------------------------|\n",
        "| 搜索方式       | 随机采样                                   |\n",
        "| 可并行计算     | ✅ 支持并行                                 |\n",
        "| 适合场景       | 超参数维度多、部分参数不重要、组合空间很大时             |\n",
        "| 效率           | ✅ 通常比 Grid Search 更快且表现接近最优           |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 优点\n",
        "\n",
        "- 减少冗余组合，提升计算效率  \n",
        "- 对高维空间更友好  \n",
        "- 易于扩展连续参数（支持均匀、对数等分布）  \n",
        "- 可用于大规模调参任务中的粗搜索\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 缺点\n",
        "\n",
        "- 有随机性：可能错过最优解  \n",
        "- 对每次采样不确定，结果可能不稳定  \n",
        "- 不适合特别小的搜索空间（Grid 更全面）\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 常见应用\n",
        "\n",
        "- 高维参数空间粗搜索（如神经网络层数、学习率）  \n",
        "- 集成算法初步调参（如 XGBoost）  \n",
        "- 搜索范围大、资源有限的调参场景\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 面试常见问题（含答案）\n",
        "\n",
        "1. **Random Search 是什么？**\n",
        "   - 一种从超参数空间中随机采样固定次数进行调参的方法。\n",
        "\n",
        "2. **Random vs Grid 哪个更好？**\n",
        "   - 随机搜索在高维空间更高效，Grid 适合小空间。\n",
        "\n",
        "3. **为什么 Random Search 更快？**\n",
        "   - 它避免了重复和不重要的组合，从整体上能更早接近最优点。\n",
        "\n",
        "4. **是否有缺点？**\n",
        "   - 可能漏掉最优解、结果具有随机性，需要多次试验保证稳定性。\n",
        "\n",
        "5. **你实际项目中用过吗？**\n",
        "   - 用于调节 Ridge 中的 alpha、XGBoost 中的 max_depth、subsample 等，快速获得有效组合。\n"
      ],
      "metadata": {
        "id": "RsYUJ12sUaqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# 加载数据\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 选择模型\n",
        "model = Ridge()\n",
        "\n",
        "# 定义参数分布（使用对数均匀分布搜索 alpha）\n",
        "param_dist = {\n",
        "    'alpha': loguniform(1e-3, 1e2)  # alpha ∈ [0.001, 100]（连续对数空间）\n",
        "}\n",
        "\n",
        "# 初始化 Random Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 训练并搜索\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# 输出结果\n",
        "print(\"最优超参数 alpha:\", random_search.best_params_['alpha'])\n",
        "print(\"最优模型在验证集上的得分 (负MSE):\", random_search.best_score_)\n",
        "\n",
        "# 测试集评估\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"在测试集上的 MSE:\", test_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydjEPgWGUepS",
        "outputId": "aa80854d-2ec3-445f-f319-6bc031ede13f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "最优超参数 alpha: 4.5705630998014515\n",
            "最优模型在验证集上的得分 (负MSE): -0.5192569425655866\n",
            "在测试集上的 MSE: 0.5554943687901324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Bayesian Optimization（贝叶斯优化）+ Optuna\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 方法介绍\n",
        "\n",
        "Bayesian Optimization 是一种**基于概率模型的超参数搜索方法**，适用于训练代价高、参数空间大、连续/离散混合等复杂调参任务。\n",
        "\n",
        "Optuna 是当前主流的贝叶斯调参库，它使用 TPE（Tree-structured Parzen Estimator）作为默认策略，并支持 early stopping、并行计算、动态图空间等。\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ 原理简述\n",
        "\n",
        "1. 构建代理模型（如 Gaussian Process / TPE）来拟合目标函数\n",
        "2. 使用采集函数（Acquisition Function）选择下一个采样点\n",
        "3. 不断优化 → 收敛到最优参数组合\n",
        "\n",
        "Optuna 中每一次尝试被称为一个 **Trial**，目标是最小化或最大化一个 **objective function**。\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 特性\n",
        "\n",
        "| 特性                 | 描述                                       |\n",
        "|----------------------|--------------------------------------------|\n",
        "| 智能采样策略         | TPE 模型预测“最值得探索”的区域               |\n",
        "| Early Stopping（剪枝） | 若中途表现差，可提前终止试验节省资源           |\n",
        "| 动态搜索空间         | 支持“运行时定义”搜索空间                     |\n",
        "| 强大的可视化         | 学习曲线、参数重要性、参数分布、一键生成报告      |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 优点\n",
        "\n",
        "- 极少试验次数内获得接近最优解  \n",
        "- 支持连续 / 离散 / 条件空间  \n",
        "- 节省计算资源（可提前停止表现差的试验）  \n",
        "- 易集成 sklearn / xgboost / lgbm 等框架  \n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 缺点\n",
        "\n",
        "- 实现与调试略复杂（需写 objective 函数）  \n",
        "- 对非常高维搜索空间仍有挑战  \n",
        "- 默认设置非最优可能导致局部最优陷阱\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 面试常见问题（含答案）\n",
        "\n",
        "1. **Bayesian Optimization 是什么？**\n",
        "   - 是通过构建概率模型近似目标函数，然后选择潜在最优区域进行采样的智能调参方法。\n",
        "\n",
        "2. **为什么选用 Optuna？**\n",
        "   - 它使用 TPE 算法效率高、支持动态空间定义、early stopping、并行计算、可视化等高级功能。\n",
        "\n",
        "3. **Optuna 和 Grid Search 有什么本质区别？**\n",
        "   - Grid 是固定穷举，Optuna 是动态学习、智能决策；后者效率更高更灵活。\n",
        "\n",
        "4. **调参时 Optuna 是怎么知道下次试什么的？**\n",
        "   - 它使用历史 trial 的结果训练代理模型，并预测哪组参数最有希望提升性能。\n",
        "\n",
        "5. **你项目中如何使用 Optuna？**\n",
        "   - 我使用 Optuna 为 Ridge/XGBoost 自动调参，用较少的 trial 达到更优验证分数，并结合 early stopping 提前终止低潜力组合。\n",
        "\n"
      ],
      "metadata": {
        "id": "VFBbJwECDyb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrkvV6fNEDjx",
        "outputId": "ff7c1985-efb9-47ff-b839-b8701174dfd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 加载数据\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 定义目标函数\n",
        "def objective(trial):\n",
        "    alpha = trial.suggest_float('alpha', 1e-3, 1e2, log=True)\n",
        "    model = Ridge(alpha=alpha)\n",
        "    # 使用 5 折交叉验证的负 MSE 作为目标\n",
        "    score = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
        "    return -1.0 * score.mean()  # Optuna 默认是最小化\n",
        "\n",
        "# 启动搜索\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# 输出最优超参数和结果\n",
        "print(\"最优 alpha:\", study.best_params['alpha'])\n",
        "print(\"验证集上最优得分（MSE）:\", study.best_value)\n",
        "\n",
        "# 使用最优超参数在测试集评估\n",
        "best_model = Ridge(alpha=study.best_params['alpha'])\n",
        "best_model.fit(X_train, y_train)\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"测试集上的 MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1h2sx4bEBt-",
        "outputId": "9dc6e023-a895-4768-a451-0e6f5b9a9f2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-31 06:48:26,390] A new study created in memory with name: no-name-afafaf55-1bb6-4c37-89ef-e3e15a7f48bf\n",
            "[I 2025-05-31 06:48:26,416] Trial 0 finished with value: 0.5192650461381854 and parameters: {'alpha': 0.06252454591840313}. Best is trial 0 with value: 0.5192650461381854.\n",
            "[I 2025-05-31 06:48:26,440] Trial 1 finished with value: 0.5192904308619919 and parameters: {'alpha': 25.812908025332064}. Best is trial 0 with value: 0.5192650461381854.\n",
            "[I 2025-05-31 06:48:26,462] Trial 2 finished with value: 0.5192651963721266 and parameters: {'alpha': 0.0019174227422827505}. Best is trial 0 with value: 0.5192650461381854.\n",
            "[I 2025-05-31 06:48:26,484] Trial 3 finished with value: 0.5192651980065923 and parameters: {'alpha': 0.0012605278589198573}. Best is trial 0 with value: 0.5192650461381854.\n",
            "[I 2025-05-31 06:48:26,511] Trial 4 finished with value: 0.5192646553605222 and parameters: {'alpha': 0.22233835373621477}. Best is trial 4 with value: 0.5192646553605222.\n",
            "[I 2025-05-31 06:48:26,533] Trial 5 finished with value: 0.5192651973902643 and parameters: {'alpha': 0.0015082250923465304}. Best is trial 4 with value: 0.5192646553605222.\n",
            "[I 2025-05-31 06:48:26,556] Trial 6 finished with value: 0.5192651749903013 and parameters: {'alpha': 0.010515703798156481}. Best is trial 4 with value: 0.5192646553605222.\n",
            "[I 2025-05-31 06:48:26,579] Trial 7 finished with value: 0.5192651977106195 and parameters: {'alpha': 0.0013794760275828058}. Best is trial 4 with value: 0.5192646553605222.\n",
            "[I 2025-05-31 06:48:26,602] Trial 8 finished with value: 0.5192651964041197 and parameters: {'alpha': 0.0019045642274656347}. Best is trial 4 with value: 0.5192646553605222.\n",
            "[I 2025-05-31 06:48:26,638] Trial 9 finished with value: 0.5192643918127642 and parameters: {'alpha': 0.33195846347742664}. Best is trial 9 with value: 0.5192643918127642.\n",
            "[I 2025-05-31 06:48:26,668] Trial 10 finished with value: 0.5192547375578064 and parameters: {'alpha': 7.718754278202045}. Best is trial 10 with value: 0.5192547375578064.\n",
            "[I 2025-05-31 06:48:26,695] Trial 11 finished with value: 0.5192546493494694 and parameters: {'alpha': 9.284659500029504}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,721] Trial 12 finished with value: 0.519259304831529 and parameters: {'alpha': 14.736834439485978}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,747] Trial 13 finished with value: 0.5192582464982587 and parameters: {'alpha': 3.556675410468345}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,778] Trial 14 finished with value: 0.5192591543802385 and parameters: {'alpha': 2.9587913753421518}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,804] Trial 15 finished with value: 0.5195656234456808 and parameters: {'alpha': 63.72616252659936}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,830] Trial 16 finished with value: 0.5192593456473008 and parameters: {'alpha': 2.84091666512544}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,856] Trial 17 finished with value: 0.5192635021828975 and parameters: {'alpha': 0.7138351039837842}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,884] Trial 18 finished with value: 0.5192553962685504 and parameters: {'alpha': 11.160392870633277}. Best is trial 11 with value: 0.5192546493494694.\n",
            "[I 2025-05-31 06:48:26,911] Trial 19 finished with value: 0.5199650255903189 and parameters: {'alpha': 98.08515320189423}. Best is trial 11 with value: 0.5192546493494694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最优 alpha: 9.284659500029504\n",
            "验证集上最优得分（MSE）: 0.5192546493494694\n",
            "测试集上的 MSE: 0.5550992550930101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧾 Grid Search vs Random Search vs Bayesian Optimization 总结\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 三种调参方法横向对比\n",
        "\n",
        "| 维度                     | Grid Search             | Random Search            | Bayesian Optimization (Optuna)    |\n",
        "|--------------------------|-------------------------|--------------------------|-----------------------------------|\n",
        "| 搜索方式                 | 穷举所有组合             | 随机采样                 | 代理模型指导采样（如 TPE）         |\n",
        "| 是否智能                 | ❌ 不智能                | ⚠️ 半智能                 | ✅ 高度智能                         |\n",
        "| 效率                     | ❌ 慢，组合爆炸            | ✅ 高效                  | ✅ 通常更快                         |\n",
        "| 高维参数空间效果         | ❌ 非常差                 | ✅ 稳定                  | ✅ 非常适合                         |\n",
        "| 是否支持 Early Stopping | ❌ 不支持                 | ❌ 不支持                 | ✅ 支持（剪枝）                     |\n",
        "| 是否可自定义分布         | ⚠️ 仅手动枚举             | ✅ 支持多种分布           | ✅ 动态定义搜索逻辑                 |\n",
        "| 是否可扩展条件搜索空间   | ❌ 静态                   | ⚠️ 静态                 | ✅ 支持 if-else / 嵌套逻辑         |\n",
        "| 实现复杂度               | ✅ 简单，容易上手           | ✅ 简单                  | ⚠️ 需要写 objective 函数（稍复杂） |\n",
        "| 最佳使用场景             | 小模型、少参数              | 粗调、时间有限             | 参数多、代价高、需要智能调参时       |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 实际项目中如何选择？\n",
        "\n",
        "| 情况                              | 推荐方法               |\n",
        "|-----------------------------------|------------------------|\n",
        "| 参数少，调参任务简单              | Grid Search           |\n",
        "| 参数多但时间有限                  | Random Search         |\n",
        "| 模型训练时间长，资源昂贵          | Bayesian Optimization |\n",
        "| 参数空间中有连续型 + 条件型参数   | Bayesian Optimization |\n",
        "| 项目初期快速验证                  | Random Search         |\n",
        "| 项目后期精细微调                  | Optuna（或 Grid 精调）|\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 面试常见问题答法示例\n",
        "\n",
        "**Q: 你项目中用什么调参方法？为什么？**  \n",
        "> 我根据任务复杂度选择不同方法。小模型用 Grid Search，粗调用 Random，更复杂任务用 Optuna 做贝叶斯优化，结合 Early Stopping 提高效率。\n",
        "\n",
        "**Q: 为什么说 Random 比 Grid 更好？**  \n",
        "> 因为在高维空间中，大部分 Grid 组合是冗余的。Random Search 能用更少的尝试覆盖更多有效区域。\n",
        "\n",
        "**Q: Optuna 优势在哪？**  \n",
        "> 它能学习历史 trial 的表现，智能决定下一步采样点；支持连续、离散、条件空间，并提供剪枝机制节省计算资源。\n",
        "\n",
        "---\n",
        "\n",
        "✅ 建议：面试时结合实际例子（如调 Ridge alpha，XGBoost 的 max_depth、learning_rate）具体说出你使用的策略会更有说服力。\n"
      ],
      "metadata": {
        "id": "A0wd0jf9HoRL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krZtWt8iHts9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}